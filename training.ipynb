{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import MSELoss\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import splitext\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from unet_model import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TheDataset(Dataset):\n",
    "    def __init__(self, interlaced_dir, gtruth_dir, scale=1):\n",
    "        self.interlaced_dir = interlaced_dir\n",
    "        self.gtruth_dir = gtruth_dir\n",
    "        self.scale = scale\n",
    "        self.transform = transforms.Compose([transforms.RandomCrop(256), transforms.ToTensor()])\n",
    "        \n",
    "        assert 0 < scale <= 1, 'Scale must be between 0 and 1'\n",
    "\n",
    "        self.ids = [splitext(file)[0] for file in listdir(interlaced_dir)]\n",
    "        logging.info(f'Creating dataset with {len(self.ids)} examples')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess(cls, pil_img, scale):\n",
    "        pil_img = pil_img / 255.0\n",
    "        \n",
    "        return pil_img\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.ids[i]\n",
    "        gtruth_file = glob(self.gtruth_dir + idx + '.*')\n",
    "        interlaced_file = glob(self.interlaced_dir + idx + '.*')\n",
    "\n",
    "        assert len(gtruth_file) == 1, \\\n",
    "            f'Either no mask or multiple masks found for the ID {idx}: {gtruth_file}'\n",
    "        assert len(interlaced_file) == 1, \\\n",
    "            f'Either no image or multiple images found for the ID {idx}: {interlaced_file}'\n",
    "        gtruth = Image.open(gtruth_file[0])\n",
    "        interlaced = Image.open(interlaced_file[0])\n",
    "\n",
    "        assert interlaced.size == gtruth.size, \\\n",
    "            f'Image and mask {idx} should be the same size, but are {interlaced.size} and {gtruth.size}'\n",
    "        \n",
    "        seed = np.random.randint(2147483647) # make a seed with numpy generator \n",
    "        \n",
    "        random.seed(seed) # apply this seed to img tranfsorms\n",
    "        torch.manual_seed(seed) # needed for torchvision 0.7\n",
    "        interlaced = self.transform(interlaced)\n",
    "        interlaced = self.preprocess(interlaced, self.scale)\n",
    "        \n",
    "        random.seed(seed) # apply this seed to img tranfsorms\n",
    "        torch.manual_seed(seed) # needed for torchvision 0.7\n",
    "        gtruth = self.transform(gtruth)\n",
    "        gtruth = self.preprocess(gtruth, self.scale)\n",
    "\n",
    "        return interlaced, gtruth\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "interlaced_dir = \"./dataset/interlaced/1/\"\n",
    "gtruth_dir = \"./dataset/ground_truth/1/\"\n",
    "img_scale = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net,\n",
    "              device,\n",
    "              epochs=5,\n",
    "              batch_size=1,\n",
    "              lr=0.001,\n",
    "              val_percent=0.1,\n",
    "              save_cp=True,\n",
    "              img_scale=1):\n",
    "\n",
    "    dataset = TheDataset(interlaced_dir, gtruth_dir, img_scale)\n",
    "    n_val = int(len(dataset) * val_percent)\n",
    "    n_train = len(dataset) - n_val\n",
    "    train, val = random_split(dataset, [n_train, n_val])\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)\n",
    "#     for inter, truth in tqdm(train_loader):\n",
    "#         f, axarr = plt.subplots(1,2)\n",
    "#         axarr[0].imshow(inter[0].permute(1,2,0))\n",
    "#         axarr[1].imshow(truth[0].permute(1,2,0))\n",
    "#         break\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=1e-8)\n",
    "    criterion = MSELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "        net.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch, (interlaced, truths) in enumerate(train_loader):\n",
    "            interlaced = interlaced.to(device=device, dtype=torch.float32)\n",
    "            truths = truths.to(device=device, dtype=torch.float32)\n",
    "            \n",
    "            net_pred = net(interlaced)\n",
    "            loss = criterion(net_pred, truths)\n",
    "            epoch_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch % 100 == 0:\n",
    "                loss, current = loss.item(), batch * len(interlaced)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{n_train:>5d}]\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.171621  [    0/84930]\n",
      "loss: 0.000108  [  800/84930]\n",
      "loss: 0.000091  [ 1600/84930]\n",
      "loss: 0.000065  [ 2400/84930]\n",
      "loss: 0.000033  [ 3200/84930]\n",
      "loss: 0.000054  [ 4000/84930]\n",
      "loss: 0.000024  [ 4800/84930]\n",
      "loss: 0.000023  [ 5600/84930]\n",
      "loss: 0.000009  [ 6400/84930]\n",
      "loss: 0.000007  [ 7200/84930]\n",
      "loss: 0.000005  [ 8000/84930]\n",
      "loss: 0.000008  [ 8800/84930]\n",
      "loss: 0.000007  [ 9600/84930]\n",
      "loss: 0.000007  [10400/84930]\n",
      "loss: 0.000004  [11200/84930]\n",
      "loss: 0.000005  [12000/84930]\n",
      "loss: 0.000005  [12800/84930]\n",
      "loss: 0.000004  [13600/84930]\n",
      "loss: 0.000005  [14400/84930]\n",
      "loss: 0.000002  [15200/84930]\n",
      "loss: 0.000003  [16000/84930]\n",
      "loss: 0.000002  [16800/84930]\n",
      "loss: 0.000002  [17600/84930]\n",
      "loss: 0.000002  [18400/84930]\n",
      "loss: 0.000002  [19200/84930]\n",
      "loss: 0.000002  [20000/84930]\n",
      "loss: 0.000002  [20800/84930]\n",
      "loss: 0.000001  [21600/84930]\n",
      "loss: 0.000002  [22400/84930]\n",
      "loss: 0.000001  [23200/84930]\n",
      "loss: 0.000001  [24000/84930]\n",
      "loss: 0.000001  [24800/84930]\n",
      "loss: 0.000002  [25600/84930]\n",
      "loss: 0.000002  [26400/84930]\n",
      "loss: 0.000001  [27200/84930]\n",
      "loss: 0.000001  [28000/84930]\n",
      "loss: 0.000001  [28800/84930]\n",
      "loss: 0.000001  [29600/84930]\n",
      "loss: 0.000001  [30400/84930]\n",
      "loss: 0.000001  [31200/84930]\n",
      "loss: 0.000001  [32000/84930]\n",
      "loss: 0.000001  [32800/84930]\n",
      "loss: 0.000001  [33600/84930]\n",
      "loss: 0.000001  [34400/84930]\n",
      "loss: 0.000001  [35200/84930]\n",
      "loss: 0.000001  [36000/84930]\n",
      "loss: 0.000001  [36800/84930]\n",
      "loss: 0.000001  [37600/84930]\n",
      "loss: 0.000001  [38400/84930]\n",
      "loss: 0.000001  [39200/84930]\n",
      "loss: 0.000001  [40000/84930]\n",
      "loss: 0.000000  [40800/84930]\n",
      "loss: 0.000001  [41600/84930]\n",
      "loss: 0.000001  [42400/84930]\n",
      "loss: 0.000004  [43200/84930]\n",
      "loss: 0.000001  [44000/84930]\n",
      "loss: 0.000001  [44800/84930]\n",
      "loss: 0.000001  [45600/84930]\n",
      "loss: 0.000001  [46400/84930]\n",
      "loss: 0.000001  [47200/84930]\n",
      "loss: 0.000001  [48000/84930]\n",
      "loss: 0.000001  [48800/84930]\n",
      "loss: 0.000001  [49600/84930]\n",
      "loss: 0.000000  [50400/84930]\n",
      "loss: 0.000001  [51200/84930]\n",
      "loss: 0.000000  [52000/84930]\n",
      "loss: 0.000005  [52800/84930]\n",
      "loss: 0.000001  [53600/84930]\n",
      "loss: 0.000001  [54400/84930]\n",
      "loss: 0.000000  [55200/84930]\n",
      "loss: 0.000000  [56000/84930]\n",
      "loss: 0.000001  [56800/84930]\n",
      "loss: 0.000001  [57600/84930]\n",
      "loss: 0.000001  [58400/84930]\n",
      "loss: 0.000001  [59200/84930]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = UNet(n_channels=3, bilinear=True)\n",
    "net.to(device=device)\n",
    "train_net(net=net,\n",
    "          epochs=1,\n",
    "          batch_size=8,\n",
    "          lr=0.001,\n",
    "          device=device,\n",
    "          img_scale=1,\n",
    "          val_percent=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
